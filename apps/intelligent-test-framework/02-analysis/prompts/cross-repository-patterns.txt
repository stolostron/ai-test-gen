You are analyzing patterns across multiple stolostron repositories to understand comprehensive implementation and testing approaches for {JIRA_TICKET}. Your goal is to discover consistent patterns, identify best practices, and ensure comprehensive test coverage.

## CROSS-REPOSITORY ANALYSIS SCOPE

### Primary Analysis Targets
{DETECTED_REPOSITORIES}

### Secondary Context Repositories
- stolostron/backplane-operator
- stolostron/managedcluster-import-controller  
- stolostron/multicluster-global-hub
- stolostron/hypershift-addon-operator

## PATTERN ANALYSIS METHODOLOGY

### 1. IMPLEMENTATION PATTERN DISCOVERY

#### Backend Patterns (Go repositories)
Analyze across repositories:
- **Error Handling**: Consistent error types, wrapping, and propagation
- **Validation Logic**: Input validation, business rule enforcement
- **API Design**: REST endpoints, GraphQL schemas, request/response patterns
- **Configuration Management**: Environment variables, config files, secrets
- **Logging and Monitoring**: Structured logging, metrics, health checks
- **Testing Approaches**: Unit tests, mocks, integration test patterns

Example analysis commands:
```
"Compare error handling patterns between cluster-curator-controller and managedcluster-import-controller"
"Analyze validation logic consistency across stolostron Go repositories"
"Review API design patterns in stolostron/console and stolostron/cluster-curator-controller"
```

#### Frontend Patterns (React/TypeScript repositories)
Analyze across UI repositories:
- **Component Architecture**: Reusable components, props patterns, composition
- **State Management**: Redux patterns, context usage, data flow
- **Form Handling**: Validation, submission, error display
- **API Integration**: HTTP clients, error handling, loading states
- **UI Testing**: Component tests, integration tests, accessibility testing
- **Styling Approaches**: CSS patterns, theme usage, responsive design

#### Testing Patterns (E2E and test repositories)
Analyze across testing repositories:
- **Test Structure**: Describe/it patterns, test organization, fixtures
- **Page Objects**: Reusable selectors, actions, assertions
- **Test Data**: Mock data, test fixtures, environment setup
- **Error Scenarios**: Negative testing, edge cases, failure simulation
- **CI/CD Integration**: Test execution, reporting, parallel execution

### 2. CONSISTENCY ANALYSIS

#### Code Quality Standards
- **Linting Rules**: ESLint, golint, formatting standards
- **Documentation**: README patterns, code comments, API docs
- **Git Workflows**: Branch naming, PR templates, commit messages
- **Security Practices**: Secret handling, input sanitization, authentication

#### Integration Patterns
- **Service Communication**: REST APIs, message queues, event handling
- **Data Flow**: Request/response cycles, state synchronization
- **Error Propagation**: How errors flow between services and UI
- **Configuration Consistency**: Shared configurations, environment variables

### 3. TESTING STRATEGY HARMONIZATION

#### Test Architecture Consistency
Analyze how different repositories handle:
- **Unit Testing**: Mocking strategies, test isolation, coverage expectations
- **Integration Testing**: Service integration, database testing, external dependencies
- **E2E Testing**: User journey testing, cross-service workflows
- **Performance Testing**: Load testing, stress testing, benchmark patterns

#### Quality Gate Alignment
- **Code Coverage**: Coverage thresholds, reporting, quality metrics
- **Static Analysis**: Security scanning, dependency checking, code quality
- **Manual Testing**: Test plans, exploratory testing, user acceptance
- **Release Testing**: Staging validation, production readiness, rollback procedures

## REPOSITORY-SPECIFIC ANALYSIS

### For Each Repository, Examine:

#### 1. cluster-curator-controller
- **Core Functionality**: Cluster lifecycle management, upgrade orchestration
- **API Patterns**: Kubernetes CRDs, controllers, reconciliation loops
- **Testing Approach**: Controller testing, API validation, integration scenarios
- **Integration Points**: How it communicates with other ACM components

#### 2. console
- **UI Architecture**: React components, routing, state management
- **User Workflows**: Cluster management UI, form handling, navigation
- **Testing Strategy**: Component testing, user interaction testing
- **API Integration**: How it consumes backend services

#### 3. clc-ui-e2e
- **Test Organization**: Spec structure, test categorization, parallel execution
- **Automation Patterns**: Page objects, reusable functions, test data management
- **CI Integration**: Jenkins integration, test reporting, failure handling
- **Coverage Strategy**: Feature coverage, regression testing, smoke tests

## PATTERN SYNTHESIS REQUIREMENTS

### 1. UNIFIED TESTING APPROACH
Based on cross-repository analysis, design:
- **Consistent Test Structure**: Naming conventions, organization patterns
- **Shared Utilities**: Reusable test helpers, common assertions
- **Standard Workflows**: Setup, execution, teardown patterns
- **Error Handling**: Consistent error testing across all layers

### 2. INTEGRATION VALIDATION
Ensure tests validate:
- **End-to-End Workflows**: Complete user journeys across services
- **Service Boundaries**: API contracts, data consistency, error propagation
- **Configuration Changes**: How configuration updates propagate
- **Upgrade Scenarios**: Backward compatibility, data migration, rollback

### 3. QUALITY ASSURANCE ALIGNMENT
- **Coverage Standards**: Consistent coverage expectations across repositories
- **Quality Metrics**: Shared definition of quality, automated checks
- **Review Processes**: Code review standards, testing review criteria
- **Documentation**: Test documentation, runbook standards

## ANALYSIS OUTPUT REQUIREMENTS

### 1. PATTERN DISCOVERY REPORT
Document discovered patterns:
- **Consistent Approaches**: What works well across repositories
- **Inconsistencies**: Areas where approaches differ
- **Best Practices**: Patterns that should be adopted everywhere
- **Anti-Patterns**: Approaches that should be avoided

### 2. TESTING HARMONIZATION PLAN
- **Unified Test Architecture**: Recommended structure for all testing
- **Shared Components**: Reusable test utilities and patterns
- **Integration Strategy**: How different test types work together
- **Quality Standards**: Consistent quality expectations

### 3. IMPLEMENTATION RECOMMENDATIONS
For {JIRA_TICKET} specifically:
- **Repository-Specific Tests**: What to test in each repository
- **Integration Points**: How to test cross-repository interactions
- **Consistency Requirements**: How to align with existing patterns
- **Innovation Opportunities**: Where to improve current approaches

## ADVANCED ANALYSIS TECHNIQUES

### 1. Dynamic Code Analysis
- **Live Repository State**: Examine current implementation, not documentation
- **Recent Evolution**: Understand how patterns have evolved
- **Active Development**: Consider ongoing changes and planned improvements
- **Community Feedback**: Review issues, discussions, recent PRs

### 2. Architectural Understanding
- **Service Dependencies**: Map dependencies between repositories
- **Data Flow Analysis**: Understand how data moves through the system
- **Error Flow Mapping**: Track how errors propagate and are handled
- **Configuration Impact**: Understand configuration dependencies

### 3. Quality Trend Analysis
- **Test Evolution**: How testing approaches have improved over time
- **Coverage Trends**: Areas where coverage has improved or declined
- **Performance Patterns**: How performance testing has evolved
- **Security Evolution**: How security testing has been integrated

## OUTPUT FORMAT

Structure your cross-repository analysis as:

1. **PATTERN DISCOVERY SUMMARY** (key findings across repositories)
2. **REPOSITORY-SPECIFIC INSIGHTS** (detailed findings per repository)
3. **CONSISTENCY ANALYSIS** (alignment and discrepancies)
4. **UNIFIED TESTING STRATEGY** (harmonized approach)
5. **INTEGRATION REQUIREMENTS** (cross-repository testing needs)
6. **IMPLEMENTATION ROADMAP** (step-by-step harmonization plan)
7. **QUALITY ASSURANCE FRAMEWORK** (standards and practices)

Begin by examining the current state of each repository and identifying both consistent patterns and areas of divergence.