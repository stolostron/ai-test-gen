# Test Plan for ACM-22079: Support digest-based upgrades via ClusterCurator

## Test Case 1: Digest-Based Upgrade Success Scenarios

**Setup**: 
- ACM hub cluster (4.12+) with ClusterCurator controller deployed and healthy
- Managed cluster imported via ACM with available OpenShift updates (verify cluster version compatibility)
- Test user with cluster-admin permissions on hub cluster and edit permissions on managed cluster namespace
- Network connectivity between hub and managed cluster for upgrade operations
- Sufficient storage and resources on managed cluster for upgrade process

| Test Steps | Expected Results |
|------------|------------------|
| 1. Authenticate to ACM hub cluster and verify permissions:<br/>CLI: `oc login https://api.hub-cluster.example.com:6443 -u testuser -p password`<br/>Verify permissions: `oc auth can-i create clustercurator --all-namespaces`<br/>UI: Access ACM console → Login → User menu → Verify cluster-admin role | CLI verification: `Login successful. You have access to X projects...` and `yes` for permission check<br/>UI verification: Console accessible, user shows admin privileges in user menu |
| 2. Verify managed cluster status and available updates:<br/>CLI: `oc get managedcluster managed-cluster-1 -o jsonpath='{.status.conditions[?(@.type=="ManagedClusterConditionAvailable")].status}'`<br/>Check updates: `oc get clusterversion -n managed-cluster-1 --context=managed-cluster-1 -o jsonpath='{.status.availableUpdates[*].version}'`<br/>UI: Clusters → All Clusters → managed-cluster-1 → Overview → Available updates | CLI verification: Returns `True` for cluster status and lists available versions (e.g., "4.15.10 4.15.11")<br/>UI verification: Cluster shows "Ready" status with green indicator and available updates listed |
| 3. Create ClusterCurator with digest-enabling force annotation:<br/>CLI: Create clustercurator-digest.yaml:<br/>```yaml<br/>apiVersion: cluster.open-cluster-management.io/v1beta1<br/>kind: ClusterCurator<br/>metadata:<br/>  name: digest-upgrade-test<br/>  namespace: managed-cluster-1<br/>  annotations:<br/>    cluster.open-cluster-management.io/upgrade-allow-not-recommended-versions: "true"<br/>spec:<br/>  desiredCuration: upgrade<br/>  upgrade:<br/>    desiredUpdate: "4.15.10"<br/>    channel: "stable-4.15"<br/>```<br/>Apply: `oc apply -f clustercurator-digest.yaml`<br/>UI: Cluster lifecycle → Create ClusterCurator → Fill form → Add annotation in advanced options | CLI verification: `clustercurator.cluster.open-cluster-management.io/digest-upgrade-test created`<br/>UI verification: ClusterCurator appears in list with "Initialized" status and annotation visible in details |
| 4. Validate force annotation triggers digest-based logic:<br/>CLI: `oc get clustercurator digest-upgrade-test -n managed-cluster-1 -o jsonpath='{.metadata.annotations.cluster\.open-cluster-management\.io/upgrade-allow-not-recommended-versions}'`<br/>Check controller logs: `oc logs -n open-cluster-management deployment/clustercurator-controller | grep "digest-upgrade-test" | head -5`<br/>UI: ClusterCurator details → Annotations tab → Verify annotation present | CLI verification: Returns `"true"` and logs show "Processing upgrade with digest lookup enabled"<br/>UI verification: Annotation `upgrade-allow-not-recommended-versions: "true"` visible in metadata |
| 5. Monitor ManagedClusterView creation for digest retrieval:<br/>CLI: `oc get managedclusterview -n managed-cluster-1 -l cluster.open-cluster-management.io/curator=digest-upgrade-test --watch-only=true --timeout=60s`<br/>UI: Resources → ManagedClusterViews → Filter by curator label | CLI verification: ManagedClusterView created with name `digest-upgrade-test-cv-<hash>` within 30 seconds<br/>UI verification: New ManagedClusterView appears with curator label and "Synced" status |
| 6. Verify digest extraction from conditionalUpdates API:<br/>CLI: Wait for view sync: `oc wait --for=condition=Processing=False managedclusterview -n managed-cluster-1 -l cluster.open-cluster-management.io/curator=digest-upgrade-test --timeout=120s`<br/>Extract digest: `oc get managedclusterview -n managed-cluster-1 -l cluster.open-cluster-management.io/curator=digest-upgrade-test -o jsonpath='{.items[0].status.result.status.conditionalUpdates[?(@.version=="4.15.10")].release.image}'`<br/>UI: ManagedClusterView details → Status → ConditionalUpdates section | CLI verification: Returns digest format `quay.io/openshift-release-dev/ocp-release@sha256:abc123def456...` (64-character hex)<br/>UI verification: ConditionalUpdates shows version 4.15.10 with digest-format image |
| 7. Confirm ManagedClusterAction uses digest without force flag:<br/>CLI: `oc get managedclusteraction -n managed-cluster-1 -o yaml | grep -A15 -B5 "desiredUpdate"`<br/>Verify no force: `oc get managedclusteraction -n managed-cluster-1 -o jsonpath='{.items[0].spec.actionRequest.object.spec.upgrade.force}'`<br/>UI: Resources → ManagedClusterActions → View YAML → Locate upgrade section | CLI verification: Shows `image: quay.io/openshift-release-dev/ocp-release@sha256:...` and force field returns empty (no force used)<br/>UI verification: YAML shows digest image format and no `force: true` in upgrade spec |
| 8. Monitor upgrade progress to completion:<br/>CLI: `oc get clustercurator digest-upgrade-test -n managed-cluster-1 --watch-only=true -o jsonpath='{.status.conditions[?(@.type=="Progressing")].status}:{.status.conditions[?(@.type=="Progressing")].message}'`<br/>Final status: `oc get clustercurator digest-upgrade-test -n managed-cluster-1 -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}'`<br/>UI: ClusterCurator details → Status tab → Monitor progress bar and conditions | CLI verification: Progressing changes from "True:Upgrade initiated" to completion, Complete condition shows "True"<br/>UI verification: Progress bar reaches 100%, status shows "Completed successfully" with green checkmark |

## Test Case 2: Fallback Mechanisms and Error Handling

**Setup**:
- Same ACM hub environment as Test Case 1
- Create test scenarios with versions available only in availableUpdates (not conditionalUpdates)
- Access to ClusterCurator controller logs for troubleshooting
- Prepare invalid version formats for error testing

| Test Steps | Expected Results |
|------------|------------------|
| 1. Create ClusterCurator targeting version in availableUpdates only:<br/>CLI: Create clustercurator-fallback.yaml with `desiredUpdate: "4.15.9"` (version not in conditionalUpdates)<br/>Include force annotation: `cluster.open-cluster-management.io/upgrade-allow-not-recommended-versions: "true"`<br/>Apply: `oc apply -f clustercurator-fallback.yaml`<br/>UI: Create ClusterCurator with version 4.15.9 and force annotation | CLI verification: `clustercurator.cluster.open-cluster-management.io/fallback-test created`<br/>UI verification: ClusterCurator created with specified version, annotation visible |
| 2. Verify fallback to availableUpdates when digest unavailable in conditionalUpdates:<br/>CLI: `oc get managedclusterview -n managed-cluster-1 -o jsonpath='{.items[0].status.result.status.conditionalUpdates[?(@.version=="4.15.9")]}'`<br/>Check fallback: `oc get managedclusterview -n managed-cluster-1 -o jsonpath='{.items[0].status.result.status.availableUpdates[?(@.version=="4.15.9")].image}'`<br/>UI: ManagedClusterView → ConditionalUpdates vs AvailableUpdates comparison | CLI verification: ConditionalUpdates returns empty `[]`, availableUpdates returns digest `quay.io/openshift-release-dev/ocp-release@sha256:...`<br/>UI verification: Version 4.15.9 visible only in availableUpdates section with digest |
| 3. Validate controller logs show fallback behavior:<br/>CLI: `oc logs -n open-cluster-management deployment/clustercurator-controller --tail=50 | grep -A3 -B3 "fallback\|availableUpdates"`<br/>UI: N/A | CLI verification: Logs show "Version not found in conditionalUpdates, falling back to availableUpdates" and "Successfully extracted digest from availableUpdates"<br/>UI verification: N/A |
| 4. Test standard behavior without force annotation:<br/>CLI: Delete previous: `oc delete clustercurator -n managed-cluster-1 --all --wait=true`<br/>Create without annotation (remove annotation lines from YAML)<br/>Apply: `oc apply -f clustercurator-standard.yaml`<br/>UI: Delete all curators → Create new without force annotation checkbox | CLI verification: ClusterCurator created successfully, no special annotations in metadata<br/>UI verification: New curator visible, no upgrade-allow-not-recommended-versions annotation |
| 5. Confirm standard upgrade behavior uses force flag instead of digest:<br/>CLI: Wait for action: `oc wait --for=condition=Processing=True clustercurator standard-test -n managed-cluster-1 --timeout=120s`<br/>Check force usage: `oc get managedclusteraction -n managed-cluster-1 -o jsonpath='{.items[0].spec.actionRequest.object.spec.upgrade.force}'`<br/>Check image format: `oc get managedclusteraction -n managed-cluster-1 -o jsonpath='{.items[0].spec.actionRequest.object.spec.upgrade.image}'`<br/>UI: ManagedClusterAction YAML → Upgrade section | CLI verification: Force returns `true`, image shows tag format `quay.io/openshift-release-dev/ocp-release:4.15.10-x86_64` (not digest)<br/>UI verification: YAML shows `force: true` and tag-based image reference |
| 6. Test error handling with invalid version format:<br/>CLI: Create clustercurator-invalid.yaml with `desiredUpdate: "invalid.version.format"`<br/>Apply: `oc apply -f clustercurator-invalid.yaml`<br/>Monitor status: `oc get clustercurator invalid-test -n managed-cluster-1 -o jsonpath='{.status.conditions[?(@.type=="Failed")]}'`<br/>UI: Create curator with malformed version → Monitor status | CLI verification: ClusterCurator created but Failed condition appears with message "Invalid version format: invalid.version.format"<br/>UI verification: Curator shows "Failed" status with error message about invalid version |
| 7. Verify comprehensive error reporting and recovery:<br/>CLI: Check all conditions: `oc get clustercurator invalid-test -n managed-cluster-1 -o jsonpath='{.status.conditions[*].type}'`<br/>View detailed message: `oc get clustercurator invalid-test -n managed-cluster-1 -o jsonpath='{.status.conditions[?(@.type=="Failed")].message}'`<br/>UI: ClusterCurator details → Status → Conditions section | CLI verification: Shows conditions "Initialized Failed" and message "Version invalid.version.format not found in available or conditional updates"<br/>UI verification: Status section shows failed condition with descriptive error message and troubleshooting hint |
| 8. Validate controller logs capture complete error flow:<br/>CLI: `oc logs -n open-cluster-management deployment/clustercurator-controller --since=5m | grep -E "(invalid-test|error|digest.*failed)" | tail -10`<br/>UI: N/A | CLI verification: Logs show "Failed to parse version", "Digest lookup failed", "Setting Failed condition" with proper error context<br/>UI verification: N/A |

## Test Case 3: Multi-Cluster and Concurrent Operations

**Setup**:
- Multiple managed clusters (minimum 3: cluster-1, cluster-2, cluster-3) imported and healthy
- Each cluster in separate namespace with unique names to test isolation
- Different OpenShift versions across clusters to test version-specific behavior
- Verify sufficient cluster resources for concurrent upgrades

| Test Steps | Expected Results |
|------------|------------------|
| 1. Verify multi-cluster environment and version diversity:<br/>CLI: `oc get managedcluster -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[?(@.type==\"ManagedClusterConditionAvailable\")].status,VERSION:.status.version.kubernetes`<br/>Check namespaces: `oc get namespaces | grep -E "(cluster-[1-3])"`<br/>UI: Clusters → All Clusters → Sort by version → Verify multiple ready clusters | CLI verification: Shows 3+ clusters with "True" status and different Kubernetes versions (e.g., 1.25.x, 1.26.x)<br/>UI verification: Multiple clusters visible with different versions and all showing "Ready" status |
| 2. Create concurrent ClusterCurators with different configurations:<br/>CLI: Apply multiple curators simultaneously:<br/>`oc apply -f clustercurator-cluster1.yaml &`<br/>`oc apply -f clustercurator-cluster2.yaml &`<br/>`oc apply -f clustercurator-cluster3.yaml &`<br/>Wait for all: `wait`<br/>Verify: `oc get clustercurator -A | grep -E "(cluster-[1-3])"`<br/>UI: Create curators for different clusters with staggered timing | CLI verification: All 3 ClusterCurators created successfully in separate namespaces without conflicts<br/>UI verification: Multiple curators appear in respective cluster namespaces with "Initialized" status |
| 3. Monitor isolated ManagedClusterView creation per cluster:<br/>CLI: `oc get managedclusterview -A --sort-by='.metadata.creationTimestamp' | grep -E "(cluster-[1-3])"`<br/>Check naming isolation: `oc get managedclusterview -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,CLUSTER:.spec.clusterName`<br/>UI: Resources → ManagedClusterViews → Group by namespace | CLI verification: Each cluster has separate ManagedClusterView with unique names, no naming conflicts across namespaces<br/>UI verification: Views grouped by namespace, each targeting correct cluster without overlap |
| 4. Verify independent digest retrieval without resource conflicts:<br/>CLI: Check parallel processing: `oc get managedclusterview -A -o jsonpath='{range .items[*]}{.metadata.namespace}{"\t"}{.status.conditions[?(@.type=="Processing")].status}{"\n"}{end}'`<br/>Monitor completion: `oc get managedclusterview -A --watch-only=true | grep "Processing.*False"`<br/>UI: N/A | CLI verification: All views show independent processing status, complete without blocking each other<br/>UI verification: N/A |
| 5. Confirm isolated ManagedClusterAction creation and execution:<br/>CLI: `oc get managedclusteraction -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,TARGET:.spec.actionRequest.object.metadata.name,STATUS:.status.conditions[0].type`<br/>Verify no cross-cluster actions: `oc get managedclusteraction -n cluster-1 -o jsonpath='{.items[0].spec.actionRequest.object.spec.clusterName}'`<br/>UI: Resources → ManagedClusterActions → Verify target isolation | CLI verification: Each action targets only its intended cluster, no cross-cluster references or actions<br/>UI verification: Actions clearly associated with correct target clusters |
| 6. Monitor independent upgrade progress without interference:<br/>CLI: `oc get clustercurator -A --watch-only=true -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,PHASE:.status.conditions[?(@.type==\"Progressing\")].status`<br/>Check timing: `oc get clustercurator -A -o jsonpath='{range .items[*]}{.metadata.namespace}{"\t"}{.status.conditions[?(@.type=="Progressing")].lastTransitionTime}{"\n"}{end}'`<br/>UI: Monitor all curators in dashboard view | CLI verification: All show independent progress, different completion times proving no serialization or blocking<br/>UI verification: Progress bars update independently, no visible dependency between clusters |
| 7. Test selective cleanup without affecting other operations:<br/>CLI: Delete cluster-1 curator only: `oc delete clustercurator -n cluster-1 --all`<br/>Verify others unaffected: `oc get clustercurator -A | grep -v cluster-1`<br/>Check resource cleanup: `oc get managedclusterview,managedclusteraction -n cluster-1`<br/>UI: Delete curator from cluster-1 → Verify others continue normally | CLI verification: Only cluster-1 resources deleted, cluster-2 and cluster-3 continue with "In Progress" status and intact resources<br/>UI verification: cluster-1 curator disappears, others remain with ongoing progress indicators |
| 8. Validate complete resource isolation and namespace boundaries:<br/>CLI: Check cross-namespace resource access: `oc get managedclusterview -n cluster-2 -o jsonpath='{.items[0].spec.clusterName}'`<br/>Verify label isolation: `oc get managedclusterview -A -l cluster.open-cluster-management.io/curator --show-labels`<br/>UI: Verify resources properly scoped to namespaces | CLI verification: cluster-2 view only targets cluster-2, labels correctly scoped to respective curators and namespaces<br/>UI verification: Resource lists show proper namespace isolation, no cross-contamination visible |

## Test Case 4: RBAC and Security Validation

**Setup**:
- Create dedicated service accounts with graduated permission levels for testing
- Prepare comprehensive RBAC policies covering ClusterCurator operations
- Access to controller logs and audit logs for security validation
- Test environment with RBAC enforcement enabled

| Test Steps | Expected Results |
|------------|------------------|
| 1. Create test service accounts with different permission levels:<br/>CLI: `oc create serviceaccount clustercurator-readonly -n default`<br/>`oc create serviceaccount clustercurator-operator -n default`<br/>`oc create serviceaccount clustercurator-admin -n default`<br/>UI: N/A | CLI verification: All 3 service accounts created successfully: `serviceaccount/clustercurator-readonly created` (repeated for each)<br/>UI verification: N/A |
| 2. Create graduated ClusterRoles for permission testing:<br/>CLI: Apply RBAC configuration:<br/>```yaml<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: ClusterRole<br/>metadata:<br/>  name: clustercurator-readonly<br/>rules:<br/>- apiGroups: ["cluster.open-cluster-management.io"]<br/>  resources: ["clustercurators"]<br/>  verbs: ["get", "list", "watch"]<br/>---<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: ClusterRole<br/>metadata:<br/>  name: clustercurator-operator<br/>rules:<br/>- apiGroups: ["cluster.open-cluster-management.io"]<br/>  resources: ["clustercurators"]<br/>  verbs: ["get", "list", "watch", "create", "update", "patch"]<br/>```<br/>UI: N/A | CLI verification: ClusterRoles created with specific permission boundaries defined<br/>UI verification: N/A |
| 3. Bind roles to service accounts and test permission boundaries:<br/>CLI: `oc create clusterrolebinding clustercurator-readonly-binding --clusterrole=clustercurator-readonly --serviceaccount=default:clustercurator-readonly`<br/>`oc create clusterrolebinding clustercurator-operator-binding --clusterrole=clustercurator-operator --serviceaccount=default:clustercurator-operator`<br/>Test permissions: `oc auth can-i create clustercurators --as=system:serviceaccount:default:clustercurator-readonly`<br/>UI: N/A | CLI verification: Bindings created successfully, readonly returns `no` for create, operator returns `yes`<br/>UI verification: N/A |
| 4. Validate restricted access cannot create privileged resources:<br/>CLI: `oc auth can-i create managedclusteraction --as=system:serviceaccount:default:clustercurator-operator`<br/>`oc auth can-i create managedclusterview --as=system:serviceaccount:default:clustercurator-operator`<br/>`oc auth can-i get secrets --as=system:serviceaccount:default:clustercurator-operator -n open-cluster-management`<br/>UI: N/A | CLI verification: All return `no` - service accounts cannot directly create ManagedCluster* resources or access controller secrets<br/>UI verification: N/A |
| 5. Test ClusterCurator creation with operator-level permissions:<br/>CLI: `oc create -f clustercurator-rbac-test.yaml --as=system:serviceaccount:default:clustercurator-operator`<br/>Verify ownership: `oc get clustercurator rbac-test -n managed-cluster-1 -o jsonpath='{.metadata.annotations.kubectl\.kubernetes\.io/last-applied-configuration}' | grep -o 'system:serviceaccount:default:clustercurator-operator'`<br/>UI: N/A | CLI verification: ClusterCurator created successfully by service account, ownership properly attributed<br/>UI verification: N/A |
| 6. Confirm controller creates privileged resources on behalf of user:<br/>CLI: Wait for controller processing: `oc wait --for=condition=Processing=True clustercurator rbac-test -n managed-cluster-1 --timeout=60s`<br/>Check resource ownership: `oc get managedclusteraction -n managed-cluster-1 -o jsonpath='{.items[0].metadata.ownerReferences[0].name}'`<br/>Verify controller service account: `oc get managedclusteraction -n managed-cluster-1 -o jsonpath='{.items[0].metadata.annotations.kubectl\.kubernetes\.io/last-applied-configuration}' | grep -o 'clustercurator-controller'`<br/>UI: N/A | CLI verification: ManagedClusterAction owned by ClusterCurator, created by controller service account (not user account)<br/>UI verification: N/A |
| 7. Validate audit trail captures proper authorization flow:<br/>CLI: `oc get events -n managed-cluster-1 --field-selector involvedObject.name=rbac-test --sort-by='.firstTimestamp'`<br/>Check controller logs: `oc logs -n open-cluster-management deployment/clustercurator-controller --since=5m | grep -E "(rbac-test|authorization|serviceaccount)"`<br/>UI: N/A | CLI verification: Events show ClusterCurator creation by user, subsequent resources created by controller; logs show proper service account escalation<br/>UI verification: N/A |
| 8. Test unauthorized operations and security boundaries:<br/>CLI: Attempt privileged operation: `oc create managedclusteraction unauthorized-test --template='{"spec":{"actionRequest":{"object":{"apiVersion":"v1","kind":"Secret"}}}}' --as=system:serviceaccount:default:clustercurator-operator`<br/>Verify rejection: `echo $?`<br/>Check forbidden resources: `oc auth can-i "*" "*" --as=system:serviceaccount:default:clustercurator-operator`<br/>UI: N/A | CLI verification: Command fails with "forbidden" error (exit code 1), wildcard permission check returns `no`<br/>UI verification: N/A |
| 9. Cleanup test security resources and verify isolation:<br/>CLI: `oc delete clusterrolebinding clustercurator-readonly-binding clustercurator-operator-binding`<br/>`oc delete clusterrole clustercurator-readonly clustercurator-operator`<br/>`oc delete serviceaccount clustercurator-readonly clustercurator-operator clustercurator-admin -n default`<br/>Verify cleanup: `oc get clusterrolebinding | grep clustercurator`<br/>UI: N/A | CLI verification: All test RBAC resources deleted, no clustercurator-related bindings remain, existing operations unaffected<br/>UI verification: N/A |
