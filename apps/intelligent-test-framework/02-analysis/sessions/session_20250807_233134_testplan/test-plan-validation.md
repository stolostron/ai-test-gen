Based on my analysis of the current test plan, I'll provide an improved version that maintains the exact same table format but enhances content quality, command accuracy, and expected results precision:

| Test Steps | Expected Results |
|------------|------------------|
| **Test Case 1: Digest-Based Upgrade Success Scenarios** |  |
| **Setup**: ACM hub cluster with managed cluster imported, OpenShift 4.12+ with available updates, cluster-admin permissions verified | Environment validated and ready for digest-based upgrade testing |
| 1. Authenticate to ACM hub cluster: `oc login https://api.hub-cluster.example.com:6443 --username=testuser --password=<password>` | Authentication successful: `Logged into "https://api.hub-cluster.example.com:6443" as "testuser" using existing credentials.` |
| 2. Verify managed cluster status: `oc get managedcluster -o custom-columns=NAME:.metadata.name,AVAILABLE:.status.conditions[?@.type=='ManagedClusterConditionAvailable'].status,VERSION:.status.version.kubernetes` | Shows cluster Available=True with current k8s version displayed |
| 3. Create test namespace with proper labeling: `oc create namespace digest-upgrade-test --dry-run=client -o yaml \| oc label --local -f - test-category=digest-upgrade -o yaml \| oc apply -f -` | Namespace created: `namespace/digest-upgrade-test created` with test label applied |
| 4. Deploy ClusterCurator with digest enablement annotation: `cat << 'EOF' \| oc apply -f -`<br/>`apiVersion: cluster.open-cluster-management.io/v1beta1`<br/>`kind: ClusterCurator`<br/>`metadata:`<br/>`  name: test-digest-upgrade`<br/>`  namespace: managed-cluster-1`<br/>`  annotations:`<br/>`    cluster.open-cluster-management.io/upgrade-allow-not-recommended-versions: "true"`<br/>`spec:`<br/>`  desiredCuration: upgrade`<br/>`  upgrade:`<br/>`    desiredUpdate: "4.15.10"`<br/>`    channel: "stable-4.15"`<br/>`EOF` | ClusterCurator created: `clustercurator.cluster.open-cluster-management.io/test-digest-upgrade created` |
| 5. Validate annotation configuration: `oc get clustercurator test-digest-upgrade -n managed-cluster-1 -o jsonpath='{.metadata.annotations.cluster\.open-cluster-management\.io/upgrade-allow-not-recommended-versions}' && echo` | Returns: `true` confirming digest-based upgrade capability is enabled |
| 6. Monitor ManagedClusterView creation with timeout: `timeout 120s oc get managedclusterview -n managed-cluster-1 --watch --field-selector metadata.name!='' \| grep -m1 test-digest-upgrade` | New ManagedClusterView appears within 2 minutes: `test-digest-upgrade-cv-<random-suffix>` |
| 7. Verify digest retrieval from conditionalUpdates: `oc get managedclusterview -n managed-cluster-1 -l cluster.open-cluster-management.io/clustercurator=test-digest-upgrade -o jsonpath='{.items[0].status.result.status.conditionalUpdates[?@.version=="4.15.10"].image}' && echo` | Returns SHA256 digest format: `quay.io/openshift-release-dev/ocp-release@sha256:[64-char-hash]` (NOT tag format) |
| 8. Confirm ManagedClusterAction uses digest without force flag: `oc get managedclusteraction -n managed-cluster-1 -o jsonpath='{.items[?@.spec.actionRequest.object.spec.desiredUpdate.image].spec.actionRequest.object.spec}' \| jq '{image: .desiredUpdate.image, force: .desiredUpdate.force}'` | Shows: `{"image":"quay.io/openshift-release-dev/ocp-release@sha256:...","force":null}` proving digest is used without force |
| **Scenario 2: Fallback to availableUpdates verification** |  |
| 9. Remove test resource cleanly: `oc delete clustercurator test-digest-upgrade -n managed-cluster-1 --wait=true` | Resource deleted: `clustercurator.cluster.open-cluster-management.io "test-digest-upgrade" deleted` |
| 10. Deploy ClusterCurator targeting availableUpdates version: Update desiredUpdate to version only in availableUpdates array and reapply | ClusterCurator created targeting version from availableUpdates (not conditionalUpdates) |
| 11. Verify availableUpdates fallback behavior: `oc get managedclusterview -n managed-cluster-1 -o jsonpath='{.items[0].status.result.status.availableUpdates[?@.version=="<target-version>"].image}' && echo` | Returns digest from availableUpdates array, confirming fallback logic works correctly |
| 12. Environment cleanup: `oc delete namespace digest-upgrade-test --cascade=foreground` | Namespace and all contained resources deleted: `namespace "digest-upgrade-test" deleted` |

| Test Steps | Expected Results |
|------------|------------------|
| **Test Case 2: Tag-Based Fallback and Error Handling** |  |
| **Setup**: Same environment as Test Case 1, with registry connectivity for tag-based fallback validation | Environment ready for fallback scenario testing |
| 1. Re-authenticate to ACM hub: `oc login https://api.hub-cluster.example.com:6443 --username=testuser` | Session refreshed with valid authentication token |
| 2. Create isolated test namespace: `oc create namespace fallback-test --dry-run=client -o yaml \| oc annotate --local -f - test-type=fallback-validation -o yaml \| oc apply -f -` | Namespace created: `namespace/fallback-test created` with proper test annotation |
| 3. Deploy ClusterCurator targeting non-digest version: `cat << 'EOF' \| oc apply -f -`<br/>`apiVersion: cluster.open-cluster-management.io/v1beta1`<br/>`kind: ClusterCurator`<br/>`metadata:`<br/>`  name: test-fallback`<br/>`  namespace: managed-cluster-1`<br/>`  annotations:`<br/>`    cluster.open-cluster-management.io/upgrade-allow-not-recommended-versions: "true"`<br/>`spec:`<br/>`  desiredCuration: upgrade`<br/>`  upgrade:`<br/>`    desiredUpdate: "4.14.99"`<br/>`    channel: "stable-4.14"`<br/>`EOF` | ClusterCurator created targeting version not available in digest arrays |
| 4. Monitor progression with detailed status tracking: `oc get clustercurator test-fallback -n managed-cluster-1 --watch -o jsonpath='{.status.conditions[?@.type].type}:{.status.conditions[?@.type].message}{"\n"}'` | Shows progression: digest search attempt → fallback initiation → tag-based upgrade preparation |
| 5. Verify tag-based fallback in ManagedClusterAction: `oc get managedclusteraction -n managed-cluster-1 -o jsonpath='{.items[?@.spec.actionRequest.object.spec.desiredUpdate.image].spec.actionRequest.object.spec}' \| jq '{image: .desiredUpdate.image, force: .desiredUpdate.force}'` | Shows: `{"image":"quay.io/openshift-release-dev/ocp-release:4.14.99-x86_64","force":true}` confirming tag format with force=true |
| **Scenario 2: Standard behavior without force annotation** |  |
| 6. Clean up previous test: `oc delete clustercurator test-fallback -n managed-cluster-1 --timeout=60s` | Resource deleted cleanly within timeout period |
| 7. Deploy ClusterCurator without digest enablement: Remove force annotation from YAML and reapply | ClusterCurator created without digest discovery capability annotation |
| 8. Verify standard upgrade path behavior: `oc get managedclusteraction -n managed-cluster-1 -o jsonpath='{.items[0].spec.actionRequest.object.spec.desiredUpdate}' \| jq` | Shows standard upgrade format with `force: true` (no digest attempted due to missing annotation) |
| **Scenario 3: Comprehensive error handling validation** |  |
| 9. Test invalid version format handling: Set `desiredUpdate: "invalid.version.format"` and apply | ClusterCurator accepts invalid format but upgrade process should fail gracefully |
| 10. Validate error messaging: `oc get clustercurator test-error -n managed-cluster-1 -o jsonpath='{.status.conditions[?@.type=="UpgradeFailed"].message}' && echo` | Clear error message: `"Invalid version format 'invalid.version.format' does not match semantic versioning"` |
| 11. Complete test cleanup: `oc delete namespace fallback-test --force --grace-period=0` | All test resources removed: `namespace "fallback-test" deleted` |

| Test Steps | Expected Results |
|------------|------------------|
| **Test Case 3: Disconnected Environment and Multi-Cluster Scenarios** |  |
| **Setup**: Air-gapped ACM with mirror registry, minimum 2 managed clusters, network policies for disconnected simulation | Disconnected environment validated with mirror registry and multiple clusters |
| 1. Verify mirror registry configuration: `oc get image.config.openshift.io/cluster -o jsonpath='{.spec.registrySourcePolicy.insecureRegistryNamespaces[*]}' && echo && oc get image.config.openshift.io/cluster -o jsonpath='{.spec.registrySourcePolicy.allowedRegistries[*]}'` | Shows only internal mirror registries configured (no external registry access) |
| 2. Authenticate to disconnected hub: `oc login https://api.disconnected-hub.internal:6443 --username=testuser --certificate-authority=/etc/ssl/certs/internal-ca.crt` | Login successful to air-gapped cluster with internal CA validation |
| 3. Enumerate available managed clusters: `oc get managedcluster -o custom-columns=NAME:.metadata.name,AVAILABLE:.status.conditions[?@.type=='ManagedClusterConditionAvailable'].status,ACCEPTED:.status.conditions[?@.type=='ManagedClusterConditionAccepted'].status` | Shows ≥2 clusters with Available=True and Accepted=True status |
| 4. Deploy mirror registry compatible ClusterCurator: `cat << 'EOF' \| oc apply -f -`<br/>`apiVersion: cluster.open-cluster-management.io/v1beta1`<br/>`kind: ClusterCurator`<br/>`metadata:`<br/>`  name: mirror-test`<br/>`  namespace: cluster-1`<br/>`  annotations:`<br/>`    cluster.open-cluster-management.io/upgrade-allow-not-recommended-versions: "true"`<br/>`spec:`<br/>`  desiredCuration: upgrade`<br/>`  upgrade:`<br/>`    desiredUpdate: "4.15.10"`<br/>`    channel: "stable-4.15"`<br/>`EOF` | ClusterCurator created successfully accepting mirror registry configuration |
| 5. Validate mirror registry digest resolution: `oc get managedclusterview -n cluster-1 -o jsonpath='{.items[0].status.result.status.conditionalUpdates[?@.version=="4.15.10"].image}' \| grep -o 'mirror-registry\.internal[^"]*'` | Digest URL contains internal mirror registry hostname: `mirror-registry.internal:5000/openshift/release@sha256:...` |
| **Scenario 2: Concurrent multi-cluster upgrade orchestration** |  |
| 6. Initiate first cluster upgrade: `oc apply -f clustercurator-cluster1.yaml -n cluster-1 && oc annotate clustercurator mirror-test -n cluster-1 test-sequence=first` | First ClusterCurator created in cluster-1 namespace with sequence annotation |
| 7. Initiate second cluster upgrade simultaneously: `oc apply -f clustercurator-cluster2.yaml -n cluster-2 && oc annotate clustercurator mirror-test -n cluster-2 test-sequence=second` | Second ClusterCurator created concurrently in cluster-2 namespace |
| 8. Monitor parallel upgrade progress: `watch 'oc get clustercurator -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,PHASE:.status.conditions[-1].type,MESSAGE:.status.conditions[-1].message'` | Both clusters show independent progression without mutual blocking or resource conflicts |
| 9. Verify isolated ManagedClusterView creation: `oc get managedclusterview -A --show-labels \| grep -E "(cluster-1\|cluster-2)" \| wc -l && oc get managedclusterview -A -o custom-columns=NAMESPACE:.metadata.namespace,CLUSTER:.metadata.labels.cluster` | Each cluster has separate ManagedClusterView resources (count=2), with proper cluster labeling |
| 10. Validate non-conflicting ManagedClusterAction targeting: `oc get managedclusteraction -A -o jsonpath='{range .items[*]}{.metadata.namespace}{":"}{.metadata.name}{":"}{.spec.actionRequest.object.metadata.name}{"\n"}{end}' \| sort` | Each action targets correct cluster without naming conflicts or cross-cluster interference |
| 11. Confirm independent upgrade progression: `oc get clustercurator -A -o jsonpath='{range .items[*]}{.metadata.namespace}:{.metadata.name}:{.status.conditions[-1].type}{"\n"}{end}'` | Both show appropriate progression states (Progressing, Running, or Completed) independently |
| 12. Clean up concurrent test resources: `oc delete clustercurator mirror-test -n cluster-1 --wait=true && oc delete clustercurator mirror-test -n cluster-2 --wait=true` | All multi-cluster test resources cleaned up: both ClusterCurator resources deleted successfully |

| Test Steps | Expected Results |
|------------|------------------|
| **Test Case 4: RBAC and Security Validation** |  |
| **Setup**: Test service account with limited permissions, RBAC policies for ClusterCurator operations, managed cluster for permission validation | Security test environment configured with proper RBAC boundaries |
| 1. Create dedicated test service account: `oc create serviceaccount clustercurator-test -n default && oc annotate serviceaccount clustercurator-test -n default test-purpose=rbac-validation` | Service account created: `serviceaccount/clustercurator-test created` with test annotation |
| 2. Define limited permission ClusterRole: `cat << 'EOF' \| oc apply -f -`<br/>`apiVersion: rbac.authorization.k8s.io/v1`<br/>`kind: ClusterRole`<br/>`metadata:`<br/>`  name: clustercurator-limited`<br/>`  labels:`<br/>`    test-component: rbac-validation`<br/>`rules:`<br/>- `apiGroups: ["cluster.open-cluster-management.io"]`<br/>`  resources: ["clustercurators"]`<br/>`  verbs: ["get", "list", "create", "watch"]`<br/>- `apiGroups: [""]`<br/>`  resources: ["namespaces"]`<br/>`  verbs: ["get", "list"]`<br/>`EOF` | ClusterRole created: `clusterrole.rbac.authorization.k8s.io/clustercurator-limited created` with minimal required permissions |
| 3. Bind limited role to test service account: `oc create clusterrolebinding clustercurator-test-binding --clusterrole=clustercurator-limited --serviceaccount=default:clustercurator-test` | ClusterRoleBinding created: `clusterrolebinding.rbac.authorization.k8s.io/clustercurator-test-binding created` |
| 4. Verify service account cannot create privileged resources: `oc auth can-i create managedclusteraction --as=system:serviceaccount:default:clustercurator-test -n managed-cluster-1 && oc auth can-i create managedclusterview --as=system:serviceaccount:default:clustercurator-test -n managed-cluster-1` | Both return `no` confirming service account is properly restricted from privileged operations |
| 5. Test ClusterCurator creation with limited permissions: `oc create -f - --as=system:serviceaccount:default:clustercurator-test << 'EOF'`<br/>`apiVersion: cluster.open-cluster-management.io/v1beta1`<br/>`kind: ClusterCurator`<br/>`metadata:`<br/>`  name: rbac-test`<br/>`  namespace: managed-cluster-1`<br/>`spec:`<br/>`  desiredCuration: upgrade`<br/>`  upgrade:`<br/>`    desiredUpdate: "4.15.10"`<br/>`EOF` | ClusterCurator created successfully: `clustercurator.cluster.open-cluster-management.io/rbac-test created` (controller handles privileged operations) |
| 6. Validate controller creates privileged resources with proper permissions: `sleep 30 && oc get managedclusteraction -n managed-cluster-1 -o jsonpath='{.items[?@.metadata.ownerReferences[0].kind=="ClusterCurator"].metadata.name}' && echo` | ManagedClusterAction created by controller (not service account): shows resource name created via controller delegation |
| 7. Examine audit trail for permission validation: `oc logs -n open-cluster-management-agent-addon deployment/cluster-curator-controller --tail=20 \| grep -E "(rbac\|permission\|authorization)" \| head -3` | Logs show proper RBAC checks: authentication and authorization events for controller service account operations |
| 8. Verify unauthorized access properly blocked: `oc get clustercurator --as=system:serviceaccount:default:nonexistent-user 2>&1 \| grep -o "forbidden.*"` | Returns permission denied: `forbidden: User "system:serviceaccount:default:nonexistent-user" cannot list resource...` |
| 9. Test cross-namespace access restrictions: `oc auth can-i get clustercurator --as=system:serviceaccount:default:clustercurator-test -n unauthorized-namespace` | Returns `no` confirming namespace-level access controls work correctly |
| 10. Complete RBAC test cleanup: `oc delete clustercurator rbac-test -n managed-cluster-1 && oc delete clusterrolebinding clustercurator-test-binding && oc delete clusterrole clustercurator-limited && oc delete serviceaccount clustercurator-test -n default` | All RBAC test resources removed: `clustercurator.../rbac-test deleted`, `clusterrolebinding.../...deleted`, etc. |
